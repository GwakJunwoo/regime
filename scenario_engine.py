"""
Multi-Asset Causal Network System
Scenario Engine - State â†’ Outcome Mapping

ëª©í‘œ:
- âŒ ì˜ˆì¸¡ ì•„ë‹˜
- âœ… ì¡°ê±´ë¶€ í†µê³„
- âœ… í•´ì„ ê°€ëŠ¥

"í˜„ì¬ ì‹œì¥ êµ¬ì¡°ê°€ ê³¼ê±° ì–´ëŠ êµ­ë©´ê³¼ ìœ ì‚¬í•œì§€ ì°¾ê³ ,
ê·¸ êµ­ë©´ ì´í›„ ìì‚°ë“¤ì˜ ì¡°ê±´ë¶€ ë¶„í¬ë¥¼ ë³´ì—¬ì¤€ë‹¤"
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from scipy.spatial.distance import cosine
import warnings
warnings.filterwarnings('ignore')


class ImprovedStructureDistance:
    """ê°œì„ ëœ êµ¬ì¡° ê±°ë¦¬ í•¨ìˆ˜"""
    
    @staticmethod
    def degree_weighted_distance(s1: np.ndarray, s2: np.ndarray, 
                                 network1: pd.DataFrame, 
                                 network2: pd.DataFrame) -> float:
        """
        Degree-weighted distance
        
        Source hubì˜ ë³€í™”ì— ë” ë¯¼ê°
        
        distance = Î£ w_i Â· |S_today_i - S_past_i|
        w_i = degree centrality
        """
        # Out-degree ê³„ì‚° (ê° ìì‚°ì˜ ì˜í–¥ë ¥)
        deg1 = network1.sum(axis=1).values
        deg2 = network2.sum(axis=1).values
        
        # Degree í‰ê· ìœ¼ë¡œ ê°€ì¤‘ì¹˜
        weights = (deg1 + deg2) / 2
        weights = weights / (weights.sum() + 1e-8)  # ì •ê·œí™”
        
        # ê°€ì¤‘ ê±°ë¦¬
        weighted_diff = weights * np.abs(s1 - s2)
        
        return np.sum(weighted_diff)
    
    @staticmethod
    def directional_distance(network1: pd.DataFrame, 
                            network2: pd.DataFrame,
                            bucket_mapping: Dict[str, str]) -> Dict[str, float]:
        """
        ë°©í–¥ì„± ê±°ë¦¬ (Risk-on/off ì „í™˜ í¬ì°©)
        
        Returns:
        --------
        {
            'risk_to_safe': float,  # Risk â†’ Safe ê°•ë„ ì°¨ì´
            'safe_to_risk': float,  # Safe â†’ Risk ê°•ë„ ì°¨ì´
            'inflow_outflow_ratio': float  # ìœ ì…/ìœ ì¶œ ë¹„ìœ¨ ì°¨ì´
        }
        """
        def calculate_directional_strength(network, mapping):
            risk_to_safe = 0
            safe_to_risk = 0
            
            for source in network.index:
                for target in network.columns:
                    if source == target:
                        continue
                    
                    source_bucket = mapping.get(source, 'Unknown')
                    target_bucket = mapping.get(target, 'Unknown')
                    weight = network.loc[source, target]
                    
                    if source_bucket == 'Risk' and target_bucket == 'Safe Haven':
                        risk_to_safe += weight
                    elif source_bucket == 'Safe Haven' and target_bucket == 'Risk':
                        safe_to_risk += weight
            
            return risk_to_safe, safe_to_risk
        
        r2s_1, s2r_1 = calculate_directional_strength(network1, bucket_mapping)
        r2s_2, s2r_2 = calculate_directional_strength(network2, bucket_mapping)
        
        return {
            'risk_to_safe_diff': abs(r2s_1 - r2s_2),
            'safe_to_risk_diff': abs(s2r_1 - s2r_2),
            'direction_change': abs((r2s_1 - s2r_1) - (r2s_2 - s2r_2))
        }


class OutcomeDistributionAnalyzer:
    """ê²°ê³¼ ë¶„í¬ ë¶„ì„ê¸° - ë°©í–¥+ë¶„ìœ„ìˆ˜ë§Œ"""
    
    def __init__(self, returns_data: pd.DataFrame):
        """ìˆ˜ìµë¥  ë°ì´í„°ë¥¼ ë°›ìŒ (ê°€ê²© ì•„ë‹˜!)"""
        self.returns = returns_data
    
    def analyze_forward_outcomes(self, 
                                 reference_date: pd.Timestamp,
                                 forward_days: int = 5) -> Dict:
        """
        íŠ¹ì • ë‚ ì§œ ì´í›„ forward_daysê°„ì˜ ê²°ê³¼ ë¶„í¬
        
        Returns:
        --------
        {
            'asset': {
                'direction': 1/-1/0,
                'direction_prob': float,
                'percentiles': {25: x, 50: y, 75: z},
                'volatility_ratio': float
            }
        }
        """
        outcomes = {}
        
        # Reference ë‚ ì§œ ì°¾ê¸°
        if reference_date not in self.returns.index:
            # ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œ ì°¾ê¸°
            idx = self.returns.index.get_indexer([reference_date], method='nearest')[0]
            reference_date = self.returns.index[idx]
        
        ref_idx = self.returns.index.get_loc(reference_date)
        
        # Forward window
        if ref_idx + forward_days >= len(self.returns):
            return None
        
        forward_window = self.returns.iloc[ref_idx + 1 : ref_idx + forward_days + 1]
        
        for asset in self.returns.columns:
            returns = forward_window[asset].values
            
            # NaN ì œê±°
            returns = returns[~np.isnan(returns)]
            
            if len(returns) == 0:
                continue
            
            # ë°©í–¥ (signì˜ ìµœë¹ˆê°’)
            direction = 1 if np.sum(returns > 0) > np.sum(returns < 0) else -1
            direction_prob = max(np.sum(returns > 0), np.sum(returns < 0)) / len(returns)
            
            # ë¶„ìœ„ìˆ˜
            percentiles = {
                25: np.percentile(returns, 25),
                50: np.percentile(returns, 50),
                75: np.percentile(returns, 75)
            }
            
            # ë³€ë™ì„± (ê³¼ê±° ëŒ€ë¹„)
            past_window = self.returns.iloc[ref_idx - forward_days : ref_idx]
            past_vol = past_window[asset].std()
            forward_vol = forward_window[asset].std()
            vol_ratio = forward_vol / (past_vol + 1e-8)
            
            outcomes[asset] = {
                'direction': direction,
                'direction_prob': direction_prob,
                'percentiles': percentiles,
                'volatility_ratio': vol_ratio,
                'mean_return': np.mean(returns),  # ì°¸ê³ ìš©
                'total_return': np.sum(returns)   # ì°¸ê³ ìš©
            }
        
        return outcomes
    
    def aggregate_outcomes(self, outcome_list: List[Dict]) -> Dict:
        """
        ì—¬ëŸ¬ ìœ ì‚¬ êµ­ë©´ì˜ ê²°ê³¼ë¥¼ ì§‘ê³„
        
        Parameters:
        -----------
        outcome_list : List[Dict]
            ê° ìœ ì‚¬ ë‚ ì§œì˜ outcome dict
        
        Returns:
        --------
        {
            'asset': {
                'direction_consensus': 1/-1,
                'direction_strength': float (0~1),
                'median_return': float,
                'p25_return': float,
                'p75_return': float,
                'vol_increase_prob': float
            }
        }
        """
        if len(outcome_list) == 0:
            return {}
        
        # ìì‚° ëª©ë¡
        assets = set()
        for outcome in outcome_list:
            if outcome is not None:
                assets.update(outcome.keys())
        
        aggregated = {}
        
        for asset in assets:
            directions = []
            mean_returns = []
            vol_ratios = []
            
            for outcome in outcome_list:
                if outcome is not None and asset in outcome:
                    directions.append(outcome[asset]['direction'])
                    mean_returns.append(outcome[asset]['mean_return'])
                    vol_ratios.append(outcome[asset]['volatility_ratio'])
            
            if len(directions) == 0:
                continue
            
            # ë°©í–¥ í•©ì˜
            direction_consensus = 1 if np.sum(directions) > 0 else -1
            direction_strength = abs(np.sum(directions)) / len(directions)
            
            # ìˆ˜ìµë¥  ë¶„ìœ„ìˆ˜
            mean_returns = np.array(mean_returns)
            
            aggregated[asset] = {
                'direction_consensus': direction_consensus,
                'direction_strength': direction_strength,
                'median_return': np.median(mean_returns),
                'p25_return': np.percentile(mean_returns, 25),
                'p75_return': np.percentile(mean_returns, 75),
                'vol_increase_prob': np.sum(np.array(vol_ratios) > 1.0) / len(vol_ratios),
                'sample_size': len(directions)
            }
        
        return aggregated


class ScenarioEngine:
    """ë¯¸ë‹ˆ ì‹œë‚˜ë¦¬ì˜¤ ì—”ì§„"""
    
    def __init__(self, 
                 system,
                 processed_data: pd.DataFrame,
                 bucket_mapping: Dict[str, str]):
        self.system = system
        self.processed_data = processed_data
        self.bucket_mapping = bucket_mapping
        self.distance_calc = ImprovedStructureDistance()
        self.outcome_analyzer = OutcomeDistributionAnalyzer(processed_data)
    
    def sparsify_network(self, network: pd.DataFrame, top_k_percent: float = 0.15) -> pd.DataFrame:
        """
        Edge sparsification - ìƒìœ„ k% edgeë§Œ ìœ ì§€
        
        êµ¬ì¡° ì•ˆì •ì„± ê°œì„ 
        """
        sparse = network.copy()
        
        # ëª¨ë“  edgeë¥¼ flatten
        edges = []
        for i in network.index:
            for j in network.columns:
                if i != j:
                    edges.append(network.loc[i, j])
        
        # Threshold ê³„ì‚°
        threshold = np.percentile(edges, (1 - top_k_percent) * 100)
        
        # ì‘ì€ edge ì œê±°
        sparse[sparse < threshold] = 0
        
        return sparse
    
    def find_similar_structures_enhanced(self,
                                        current_date: pd.Timestamp,
                                        top_k: int = 10,
                                        exclude_recent_days: int = 60) -> List[Dict]:
        """
        ê°œì„ ëœ ìœ ì‚¬ êµ¬ì¡° íƒìƒ‰
        
        - Degree-weighted distance
        - Directional components
        """
        # í˜„ì¬ ë‚ ì§œì˜ êµ¬ì¡°
        current_structure = None
        current_network = None
        
        for struct in self.system.structure_history:
            if pd.Timestamp(struct['date']).normalize() == current_date.normalize():
                current_structure = struct['structure_vector']
                break
        
        for net_dict in self.system.network_history:
            if pd.Timestamp(net_dict['date']).normalize() == current_date.normalize():
                current_network = net_dict['network']
                break
        
        if current_structure is None or current_network is None:
            return []
        
        # Sparsify
        current_network_sparse = self.sparsify_network(current_network, top_k_percent=0.15)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        
        for idx, struct in enumerate(self.system.structure_history):
            date = struct['date']
            
            # ìµœê·¼ ì œì™¸
            if (current_date - date).days < exclude_recent_days:
                continue
            
            # í•´ë‹¹ ë‚ ì§œì˜ ë„¤íŠ¸ì›Œí¬ ì°¾ê¸°
            past_network = None
            for net_dict in self.system.network_history:
                if pd.Timestamp(net_dict['date']).normalize() == date.normalize():
                    past_network = net_dict['network']
                    break
            
            if past_network is None:
                continue
            
            # Sparsify
            past_network_sparse = self.sparsify_network(past_network, top_k_percent=0.15)
            
            # Degree-weighted distance
            deg_dist = self.distance_calc.degree_weighted_distance(
                current_structure,
                struct['structure_vector'],
                current_network_sparse,
                past_network_sparse
            )
            
            # Directional distance
            dir_metrics = self.distance_calc.directional_distance(
                current_network_sparse,
                past_network_sparse,
                self.bucket_mapping
            )
            
            # ì¢…í•© ê±°ë¦¬ (degree-weighted 70%, directional 30%)
            total_distance = 0.7 * deg_dist + 0.3 * dir_metrics['direction_change']
            
            similarities.append({
                'date': date,
                'distance': total_distance,
                'deg_distance': deg_dist,
                'directional_metrics': dir_metrics,
                'structure_vector': struct['structure_vector']
            })
        
        # ì •ë ¬ (ê±°ë¦¬ ì‘ì€ ìˆœ)
        similarities = sorted(similarities, key=lambda x: x['distance'])[:top_k]
        
        return similarities
    
    def generate_scenario_summary(self,
                                 current_date: pd.Timestamp,
                                 forward_days: int = 5,
                                 top_k_similar: int = 10) -> Dict:
        """
        ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½ ìƒì„± (ë”œëŸ¬ìš©)
        
        Returns:
        --------
        {
            'current_date': date,
            'similar_periods': [...],
            'hub_assets': [...],
            'structure_interpretation': str,
            'outcome_distribution': {...}
        }
        """
        print("="*80)
        print(f"ğŸ“Œ Market Structure Scenario - {current_date.strftime('%Y-%m-%d')}")
        print("="*80)
        
        # 1. ìœ ì‚¬ êµ¬ì¡° íƒìƒ‰
        similar_periods = self.find_similar_structures_enhanced(
            current_date,
            top_k=top_k_similar,
            exclude_recent_days=60
        )
        
        if len(similar_periods) == 0:
            print("âš ï¸ ìœ ì‚¬ êµ­ë©´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # 2. í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ Hub ìì‚° ì°¾ê¸°
        current_network = None
        for net_dict in self.system.network_history:
            if pd.Timestamp(net_dict['date']).normalize() == current_date.normalize():
                current_network = net_dict['network']
                break
        
        hub_assets = []
        if current_network is not None:
            out_degrees = current_network.sum(axis=1).sort_values(ascending=False)
            hub_assets = [
                {
                    'asset': asset,
                    'strength': strength,
                    'bucket': self.bucket_mapping.get(asset, 'Unknown')
                }
                for asset, strength in out_degrees.head(5).items()
            ]
        
        # 3. êµ¬ì¡° í•´ì„ (ë°©í–¥ì„±)
        dir_metrics = similar_periods[0]['directional_metrics']
        
        interpretation = []
        if dir_metrics['risk_to_safe_diff'] > dir_metrics['safe_to_risk_diff']:
            interpretation.append("Risk-off ì „í™˜ êµ¬ì¡°")
        else:
            interpretation.append("Risk-on êµ¬ì¡°")
        
        # Hub bucket í•´ì„
        if len(hub_assets) > 0:
            top_bucket = hub_assets[0]['bucket']
            interpretation.append(f"ì£¼ìš” ì •ë³´ í—ˆë¸Œ: {top_bucket}")
        
        # 4. ê³¼ê±° ìœ ì‚¬ êµ­ë©´ ì´í›„ ê²°ê³¼ ìˆ˜ì§‘
        outcome_list = []
        for period in similar_periods:
            outcome = self.outcome_analyzer.analyze_forward_outcomes(
                period['date'],
                forward_days=forward_days
            )
            if outcome is not None:
                outcome_list.append(outcome)
        
        # 5. ê²°ê³¼ ì§‘ê³„
        aggregated_outcomes = self.outcome_analyzer.aggregate_outcomes(outcome_list)
        
        # 6. ì¶œë ¥
        print("\nâ€¢ ìœ ì‚¬ êµ­ë©´:")
        for i, period in enumerate(similar_periods[:5], 1):
            print(f"  {i}. {period['date'].strftime('%Y-%m-%d')} (ê±°ë¦¬: {period['distance']:.4f})")
        
        print("\nâ€¢ ì£¼ìš” ì •ë³´ í—ˆë¸Œ (Out-degree ìƒìœ„ 5ê°œ):")
        for hub in hub_assets:
            print(f"  - {hub['asset']} [{hub['bucket']}]: {hub['strength']:.3f}")
        
        print("\nâ€¢ êµ¬ì¡° í•´ì„:")
        for interp in interpretation:
            print(f"  - {interp}")
        
        print(f"\nğŸ“Š ê³¼ê±° í•´ë‹¹ êµ­ë©´ ì´í›„ {forward_days}ì¼ ê²°ê³¼ (ì¡°ê±´ë¶€ ë¶„í¬):\n")
        
        # ìì‚°ë³„ ê²°ê³¼ (bucketë³„ë¡œ ê·¸ë£¹í™”)
        bucket_outcomes = {}
        for asset, outcome in aggregated_outcomes.items():
            bucket = self.bucket_mapping.get(asset, 'Unknown')
            if bucket not in bucket_outcomes:
                bucket_outcomes[bucket] = []
            bucket_outcomes[bucket].append((asset, outcome))
        
        for bucket in ['Rates', 'Risk', 'Safe Haven', 'FX', 'Commodities']:
            if bucket not in bucket_outcomes:
                continue
            
            print(f"\n[{bucket}]")
            
            for asset, outcome in bucket_outcomes[bucket]:
                direction_symbol = "â†‘" if outcome['direction_consensus'] == 1 else "â†“"
                strength_pct = outcome['direction_strength'] * 100
                
                # ìˆ˜ìµë¥ ì€ ì´ë¯¸ normalized returnsì´ë¯€ë¡œ basis pointë¡œ í‘œì‹œ
                print(f"  â€¢ {asset}")
                print(f"    ë°©í–¥: {direction_symbol} (í™•ë¥  {strength_pct:.0f}%)")
                print(f"    ì¤‘ì•™ê°’: {outcome['median_return']:+.4f}")
                print(f"    ë²”ìœ„: [{outcome['p25_return']:+.4f}, {outcome['p75_return']:+.4f}]")
                print(f"    ë³€ë™ì„± ì¦ê°€ í™•ë¥ : {outcome['vol_increase_prob']*100:.0f}%")
                print(f"    ìƒ˜í”Œ: {outcome['sample_size']}ê°œ êµ­ë©´")
        
        print("\n" + "="*80)
        print("âœ… ì´ê²ƒì€ ì˜ˆì¸¡ì´ ì•„ë‹Œ 'ê³¼ê±° ìœ ì‚¬ êµ­ë©´ì˜ ì¡°ê±´ë¶€ í†µê³„'ì…ë‹ˆë‹¤.")
        print("="*80)
        
        return {
            'current_date': current_date,
            'similar_periods': similar_periods,
            'hub_assets': hub_assets,
            'structure_interpretation': interpretation,
            'outcome_distribution': aggregated_outcomes,
            'sample_size': len(outcome_list)
        }


if __name__ == "__main__":
    from main_system import MultiAssetCausalSystem
    
    print("ì‹œìŠ¤í…œ ë¡œë”© ì¤‘...")
    
    # ìºì‹œì—ì„œ ë¡œë“œ
    cached = MultiAssetCausalSystem.load_system("./results/system_cache.pkl")
    if cached is not None:
        system, _ = cached
        print("âœ“ ìºì‹œì—ì„œ ë¡œë“œ ì™„ë£Œ\n")
    else:
        print("âš ï¸ ìºì‹œ ì—†ìŒ. ë¨¼ì € ì‹œìŠ¤í…œì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
        exit(1)
    
    # Scenario Engine ìƒì„± (processed_data ì‚¬ìš©!)
    engine = ScenarioEngine(
        system=system,
        processed_data=system.processed_data,
        bucket_mapping=system.bucket_mapping
    )
    
    # ìµœê·¼ ë‚ ì§œë¡œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
    latest_date = system.network_history[-1]['date']
    
    scenario = engine.generate_scenario_summary(
        current_date=latest_date,
        forward_days=5,
        top_k_similar=10
    )
