"""
Multi-Asset Causal Network System
Validation Framework - ë”œëŸ¬/ë¦¬ì„œì¹˜/ë¦¬ìŠ¤í¬ ê²€ì¦ìš©
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

from main_system import MultiAssetCausalSystem
from sklearn.cluster import KMeans
from scipy.spatial.distance import cosine
from datetime import timedelta


class ValidationFramework:
    """ê²€ì¦ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self, system: MultiAssetCausalSystem):
        self.system = system
        
        # ê²½ì œì  ë…¼ë¦¬ ê·œì¹™ (ë°©í–¥ì„±)
        self.plausible_edges = {
            ('Rates', 'Risk'): True,      # ê¸ˆë¦¬ â†’ ìœ„í—˜ìì‚° â­•
            ('Risk', 'Rates'): False,     # ìœ„í—˜ìì‚° â†’ ê¸ˆë¦¬ âŒ
            ('FX', 'Risk'): True,         # USD â†’ EM Risk â­•
            ('Safe Haven', 'Risk'): True, # ì•ˆì „ìì‚° â†’ ìœ„í—˜ìì‚° â­•
            ('Risk', 'Safe Haven'): True, # Risk-off ì‹œ â­•
            ('Commodities', 'Risk'): True,
            ('Safe Haven', 'Rates'): 'conditional'  # âš ï¸ ìƒí™©ë¶€
        }
    
    # =========================================================================
    # 1ï¸âƒ£ ì‹œê°„ì  ì„ í–‰ì„± ê²€ì¦ (Temporal Validity)
    # =========================================================================
    
    def test_temporal_validity(self, forward_days: int = 5, top_k: int = 5):
        """
        ê°€ì¥ ì¤‘ìš”í•œ ê²€ì¦: Source ìì‚°ì´ ì‹¤ì œë¡œ Targetì„ ì„ í–‰í•˜ëŠ”ê°€?
        
        Returns:
        --------
        dict: {
            'source_asset': {
                'target_vol_increase': bool,
                'direction_consistency': float,
                'variance_ratio': float
            }
        }
        """
        print("="*80)
        print("1ï¸âƒ£ ì‹œê°„ì  ì„ í–‰ì„± ê²€ì¦ (Temporal Validity)")
        print("="*80)
        print(f"ë¶„ì„ ê¸°ê°„: t+1 ~ t+{forward_days}ì¼\n")
        
        results = {}
        
        for idx, net_dict in enumerate(self.system.network_history):
            if idx + forward_days >= len(self.system.network_history):
                break
            
            date = net_dict['date']
            network = net_dict['network']
            
            # ê° ë‚ ì§œì˜ Top-K ê°•í•œ ì—°ê²° ì¶”ì¶œ
            edges = []
            for source in network.index:
                for target in network.columns:
                    if source != target:
                        weight = network.loc[source, target]
                        if weight > 0:
                            edges.append({
                                'source': source,
                                'target': target,
                                'weight': weight,
                                'source_bucket': self.system.bucket_mapping.get(source),
                                'target_bucket': self.system.bucket_mapping.get(target)
                            })
            
            edges = sorted(edges, key=lambda x: x['weight'], reverse=True)[:top_k]
            
            # ê° edgeì— ëŒ€í•´ forward validation
            for edge in edges:
                target_asset = edge['target']
                
                # t+1 ~ t+forward_days êµ¬ê°„ì˜ Target ìì‚° ë³€ë™ì„±
                future_returns = []
                for i in range(1, forward_days + 1):
                    if idx + i < len(self.system.network_history):
                        future_date = self.system.network_history[idx + i]['date']
                        if future_date in self.system.processed_data.index and target_asset in self.system.processed_data.columns:
                            future_returns.append(
                                self.system.processed_data.loc[future_date, target_asset]
                            )
                
                if len(future_returns) > 0:
                    future_vol = np.std(future_returns)
                    
                    # ê³¼ê±° ë³€ë™ì„±ê³¼ ë¹„êµ (baseline)
                    past_returns = []
                    for i in range(1, forward_days + 1):
                        if idx - i >= 0:
                            past_date = self.system.network_history[idx - i]['date']
                            if past_date in self.system.processed_data.index and target_asset in self.system.processed_data.columns:
                                past_returns.append(
                                    self.system.processed_data.loc[past_date, target_asset]
                                )
                    
                    if len(past_returns) > 0:
                        past_vol = np.std(past_returns)
                        variance_ratio = future_vol / (past_vol + 1e-8)
                        
                        edge_key = f"{edge['source']} â†’ {target_asset}"
                        if edge_key not in results:
                            results[edge_key] = {
                                'count': 0,
                                'vol_increase_count': 0,
                                'variance_ratios': []
                            }
                        
                        results[edge_key]['count'] += 1
                        results[edge_key]['variance_ratios'].append(variance_ratio)
                        if variance_ratio > 1.0:
                            results[edge_key]['vol_increase_count'] += 1
        
        # ê²°ê³¼ ì •ë¦¬
        print("\nğŸ“Š ì£¼ìš” ì¸ê³¼ê´€ê³„ì˜ ì‹œê°„ì  íƒ€ë‹¹ì„±:\n")
        print(f"{'Source â†’ Target':<40} {'ë°œìƒíšŸìˆ˜':>8} {'Vol ì¦ê°€ìœ¨':>12} {'í‰ê·  ë¶„ì‚°ë¹„':>12}")
        print("-" * 80)
        
        sorted_results = sorted(results.items(), key=lambda x: x[1]['count'], reverse=True)[:15]
        
        for edge_key, stats in sorted_results:
            vol_increase_rate = stats['vol_increase_count'] / stats['count'] * 100
            avg_variance_ratio = np.mean(stats['variance_ratios'])
            
            print(f"{edge_key:<40} {stats['count']:>8} {vol_increase_rate:>11.1f}% {avg_variance_ratio:>12.3f}")
        
        print(f"\nâœ… ê¸°ì¤€: Vol ì¦ê°€ìœ¨ > 50%, ë¶„ì‚°ë¹„ > 1.0 ì´ë©´ ì‹œê°„ì  ì„ í–‰ì„± í™•ì¸")
        
        return results
    
    # =========================================================================
    # 2ï¸âƒ£ êµ¬ì¡° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (Structural Stability)
    # =========================================================================
    
    def test_structural_stability(self, n_clusters: int = 5):
        """
        ë¹„ìŠ·í•œ ì‹œì¥ êµ­ë©´ì—ì„œ ë¹„ìŠ·í•œ êµ¬ì¡°ê°€ ë‚˜ì˜¤ëŠ”ê°€?
        
        ìœ„ê¸° ì‹œì ë“¤ë¼ë¦¬ í´ëŸ¬ìŠ¤í„°ë§ë˜ë©´ ì„±ê³µ
        """
        print("\n" + "="*80)
        print("2ï¸âƒ£ êµ¬ì¡° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (Structural Stability)")
        print("="*80)
        
        # Structure vector ìˆ˜ì§‘
        dates = []
        vectors = []
        
        for struct in self.system.structure_history:
            dates.append(struct['date'])
            vectors.append(struct['structure_vector'])
        
        vectors = np.array(vectors)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(vectors)
        
        # ì•Œë ¤ì§„ ìœ„ê¸° êµ¬ê°„ ì •ì˜
        crisis_periods = {
            '2020 COVID': ('2020-02-01', '2020-04-30'),
            '2022 ê¸´ì¶•': ('2022-09-01', '2022-11-30'),
            '2018 ë³€ë™ì„±': ('2018-10-01', '2018-12-31'),
        }
        
        print("\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ì‹œì :\n")
        
        cluster_dates = {}
        for cluster_id in range(n_clusters):
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_dates[cluster_id] = [dates[i] for i in cluster_indices]
            
            print(f"í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_indices)}ê°œ ì‹œì ):")
            
            # ëŒ€í‘œ ë‚ ì§œ ìƒ˜í”Œ ì¶œë ¥
            sample_dates = cluster_dates[cluster_id][:5]
            for d in sample_dates:
                print(f"  - {d.strftime('%Y-%m-%d')}")
            
            # ìœ„ê¸° êµ¬ê°„ê³¼ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            crisis_overlap = {}
            for crisis_name, (start, end) in crisis_periods.items():
                start_date = pd.Timestamp(start)
                end_date = pd.Timestamp(end)
                
                overlap_count = sum(1 for d in cluster_dates[cluster_id] 
                                   if start_date <= d <= end_date)
                
                if overlap_count > 0:
                    crisis_overlap[crisis_name] = overlap_count
            
            if crisis_overlap:
                print(f"  ğŸ“Œ ìœ„ê¸° êµ¬ê°„ í¬í•¨: {crisis_overlap}")
            
            print()
        
        print("âœ… ì„±ê³µ ê¸°ì¤€: ìœ„ê¸° ì‹œì ë“¤ì´ ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— ëª¨ì´ë©´ í•©ê²©\n")
        
        return {
            'labels': labels,
            'cluster_dates': cluster_dates,
            'n_clusters': n_clusters
        }
    
    # =========================================================================
    # 3ï¸âƒ£ ì´ë²¤íŠ¸ ì¡°ê±´ë¶€ ê²€ì¦ (Event-based Sanity Check)
    # =========================================================================
    
    def test_event_sensitivity(self):
        """
        ì•Œë ¤ì§„ ì´ë²¤íŠ¸ ì „í›„ë¡œ ë„¤íŠ¸ì›Œí¬ê°€ ë°”ë€Œì—ˆëŠ”ê°€?
        
        FOMC â†’ Rates ì¤‘ì‹¬
        Risk-off â†’ Safe Haven inbound ì¦ê°€
        """
        print("="*80)
        print("3ï¸âƒ£ ì´ë²¤íŠ¸ ì¡°ê±´ë¶€ ê²€ì¦ (Event-based Sanity Check)")
        print("="*80)
        
        # ì£¼ìš” ì´ë²¤íŠ¸ ì •ì˜
        events = {
            '2022 FOMC (11/2)': pd.Timestamp('2022-11-02'),
            '2020 COVID ê¸‰ë½': pd.Timestamp('2020-03-16'),
            '2023 SVB íŒŒì‚°': pd.Timestamp('2023-03-10'),
        }
        
        print("\nğŸ“Š ì´ë²¤íŠ¸ ì „í›„ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë³€í™”:\n")
        
        for event_name, event_date in events.items():
            print(f"ì´ë²¤íŠ¸: {event_name}")
            
            # ì´ë²¤íŠ¸ ì§ì „/ì§í›„ ë„¤íŠ¸ì›Œí¬ ì°¾ê¸°
            before_net = None
            after_net = None
            
            for net_dict in self.system.network_history:
                date = net_dict['date']
                if date < event_date and (event_date - date).days <= 5:
                    before_net = net_dict['network']
                elif date >= event_date and (date - event_date).days <= 5:
                    after_net = net_dict['network']
                    break
            
            if before_net is not None and after_net is not None:
                # Bucketë³„ ì˜í–¥ë ¥ ë³€í™”
                before_bucket_strength = self._calculate_bucket_strength(before_net)
                after_bucket_strength = self._calculate_bucket_strength(after_net)
                
                print(f"  êµ¬ì¡° ë³€í™”:")
                for bucket in before_bucket_strength.keys():
                    before_val = before_bucket_strength[bucket]
                    after_val = after_bucket_strength[bucket]
                    change = after_val - before_val
                    
                    arrow = "â†‘" if change > 0.01 else "â†“" if change < -0.01 else "â†’"
                    print(f"    {bucket:15} {before_val:.3f} â†’ {after_val:.3f} {arrow}")
                
                print()
            else:
                print(f"  âš ï¸ ë°ì´í„° ì—†ìŒ\n")
        
        print("âœ… ê¸°ì¤€: FOMC â†’ Rates ì¦ê°€, Risk-off â†’ Safe Haven ì¦ê°€ í™•ì¸\n")
    
    def _calculate_bucket_strength(self, network):
        """Bucketë³„ ì´ out-strength ê³„ì‚°"""
        bucket_strength = {}
        
        for source in network.index:
            source_bucket = self.system.bucket_mapping.get(source, 'Unknown')
            strength = network.loc[source, :].sum()
            
            if source_bucket not in bucket_strength:
                bucket_strength[source_bucket] = 0
            bucket_strength[source_bucket] += strength
        
        # ì •ê·œí™”
        total = sum(bucket_strength.values())
        if total > 0:
            bucket_strength = {k: v/total for k, v in bucket_strength.items()}
        
        return bucket_strength
    
    # =========================================================================
    # 4ï¸âƒ£ ë°©í–¥ì„± ë…¼ë¦¬ ê²€ì¦ (Economic Plausibility)
    # =========================================================================
    
    def test_economic_plausibility(self):
        """
        ê²½ì œì ìœ¼ë¡œ ë§ì´ ë˜ëŠ” ë°©í–¥ì¸ê°€?
        
        Rates â†’ Equity â­•
        Equity â†’ Rates âŒ
        """
        print("="*80)
        print("4ï¸âƒ£ ë°©í–¥ì„± ë…¼ë¦¬ ê²€ì¦ (Economic Plausibility)")
        print("="*80)
        
        violation_count = 0
        total_edges = 0
        
        violations = []
        
        for net_dict in self.system.network_history:
            network = net_dict['network']
            date = net_dict['date']
            
            for source in network.index:
                for target in network.columns:
                    if source == target:
                        continue
                    
                    weight = network.loc[source, target]
                    if weight > 0.01:  # ìœ ì˜í•œ ì—°ê²°ë§Œ
                        total_edges += 1
                        
                        source_bucket = self.system.bucket_mapping.get(source, 'Unknown')
                        target_bucket = self.system.bucket_mapping.get(target, 'Unknown')
                        
                        edge_key = (source_bucket, target_bucket)
                        
                        if edge_key in self.plausible_edges:
                            if self.plausible_edges[edge_key] == False:
                                violation_count += 1
                                violations.append({
                                    'date': date,
                                    'source': source,
                                    'target': target,
                                    'source_bucket': source_bucket,
                                    'target_bucket': target_bucket,
                                    'weight': weight
                                })
        
        print(f"\nğŸ“Š ê²½ì œì  ë…¼ë¦¬ ìœ„ë°° ë¶„ì„:\n")
        print(f"ì´ ìœ ì˜í•œ ì—£ì§€: {total_edges}")
        print(f"ë…¼ë¦¬ ìœ„ë°° ì—£ì§€: {violation_count}")
        print(f"ìœ„ë°°ìœ¨: {violation_count/total_edges*100:.2f}%\n")
        
        if len(violations) > 0:
            print("âŒ ì£¼ìš” ìœ„ë°° ì‚¬ë¡€ (ìƒìœ„ 10ê°œ):\n")
            violations = sorted(violations, key=lambda x: x['weight'], reverse=True)[:10]
            
            for v in violations:
                print(f"  {v['date'].strftime('%Y-%m-%d')}: {v['source']} â†’ {v['target']} "
                      f"[{v['source_bucket']} â†’ {v['target_bucket']}] (weight: {v['weight']:.3f})")
        else:
            print("âœ… ë…¼ë¦¬ ìœ„ë°° ì—†ìŒ!")
        
        print(f"\nâœ… ê¸°ì¤€: ìœ„ë°°ìœ¨ < 10% ì´ë©´ í•©ê²©\n")
        
        return {
            'total_edges': total_edges,
            'violations': violation_count,
            'violation_rate': violation_count/total_edges if total_edges > 0 else 0
        }
    
    # =========================================================================
    # 5ï¸âƒ£ ë°˜ì‚¬ì‹¤ í…ŒìŠ¤íŠ¸ (Perturbation Test)
    # =========================================================================
    
    def test_perturbation(self, target_asset: str = 'ì½”ìŠ¤í”¼200 ì—°ê²°'):
        """
        íŠ¹ì • ìì‚° returnì„ 0ìœ¼ë¡œ ê³ ì •í•˜ë©´ ë„¤íŠ¸ì›Œí¬ê°€ í•©ë¦¬ì ìœ¼ë¡œ ë°”ë€Œë‚˜?
        
        ê¸°ëŒ€: í•´ë‹¹ ìì‚°ì˜ out-degree ê°ì†Œ
        """
        print("="*80)
        print("5ï¸âƒ£ ë°˜ì‚¬ì‹¤ í…ŒìŠ¤íŠ¸ (Perturbation Test)")
        print("="*80)
        print(f"ëŒ€ìƒ ìì‚°: {target_asset}\n")
        
        # ì›ë³¸ ë°ì´í„° ë³µì‚¬
        perturbed_data = self.system.processed_data.copy()
        
        # Target ìì‚°ì„ 0ìœ¼ë¡œ ê³ ì • (ë³€ë™ì„±ì€ ìœ ì§€)
        if target_asset in perturbed_data.columns:
            perturbed_data[target_asset] = 0
        else:
            print(f"âš ï¸ ìì‚° '{target_asset}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ ìƒì„± (ìµœê·¼ 200ì¼ë§Œ)
        print("ì›ë³¸ ë„¤íŠ¸ì›Œí¬ ì¬êµ¬ì„± ì¤‘...")
        original_network = self.system.network_history[-1]['network']
        
        print("êµë€ëœ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")
        # ê°„ë‹¨í•œ ì¬ìƒì„± (Granger ëŒ€ì‹  correlation ì‚¬ìš©)
        from causal_network import CausalNetworkModel
        
        temp_model = CausalNetworkModel(max_lag=3)
        perturbed_network = temp_model.correlation_network(
            perturbed_data.iloc[-120:],  # ìµœê·¼ 120ì¼
            use_lag=True
        )
        
        # Out-degree ë¹„êµ
        original_out = original_network.loc[target_asset, :].sum()
        perturbed_out = perturbed_network.loc[target_asset, :].sum()
        
        print(f"\nğŸ“Š {target_asset}ì˜ ì˜í–¥ë ¥ ë³€í™”:\n")
        print(f"ì›ë³¸ out-degree:     {original_out:.4f}")
        print(f"êµë€ í›„ out-degree:  {perturbed_out:.4f}")
        print(f"ê°ì†Œìœ¨:              {(1 - perturbed_out/original_out)*100:.1f}%\n")
        
        # ì „ì²´ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„±
        total_edges_original = (original_network > 0.01).sum().sum()
        total_edges_perturbed = (perturbed_network > 0.01).sum().sum()
        
        print(f"ì „ì²´ ìœ ì˜í•œ ì—£ì§€ ìˆ˜:")
        print(f"  ì›ë³¸:    {total_edges_original}")
        print(f"  êµë€ í›„: {total_edges_perturbed}")
        print(f"  ë³€í™”ìœ¨:  {(total_edges_perturbed/total_edges_original - 1)*100:+.1f}%\n")
        
        print("âœ… ê¸°ì¤€: Out-degree 30% ì´ìƒ ê°ì†Œ + ì „ì²´ êµ¬ì¡° ë¶•ê´´ ì—†ìŒ(Â±20% ì´ë‚´)\n")
        
        return {
            'original_out_degree': original_out,
            'perturbed_out_degree': perturbed_out,
            'reduction_rate': (1 - perturbed_out/original_out),
            'network_stability': total_edges_perturbed/total_edges_original
        }
    
    # =========================================================================
    # 6ï¸âƒ£ ê°„ì´ ì˜ˆì¸¡ ê²€ì¦ (Auxiliary Forecast Test)
    # =========================================================================
    
    def test_auxiliary_forecast(self, forward_days: int = 5):
        """
        High connectivity day â†’ ì´í›„ ë³€ë™ì„± â†‘
        Low connectivity day â†’ ì´í›„ ë³€ë™ì„± â†“
        """
        print("="*80)
        print("6ï¸âƒ£ ê°„ì´ ì˜ˆì¸¡ ê²€ì¦ (Auxiliary Forecast Test)")
        print("="*80)
        
        # ê° ë‚ ì§œì˜ ë„¤íŠ¸ì›Œí¬ ë°€ë„ ê³„ì‚°
        densities = []
        future_vols = []
        
        for idx, net_dict in enumerate(self.system.network_history):
            if idx + forward_days >= len(self.system.network_history):
                break
            
            network = net_dict['network']
            density = (network > 0.01).sum().sum() / (network.shape[0] * network.shape[1])
            
            densities.append(density)
            
            # ì´í›„ 5ì¼ê°„ í‰ê·  ë³€ë™ì„±
            future_returns = []
            for i in range(1, forward_days + 1):
                future_date = self.system.network_history[idx + i]['date']
                if future_date in self.system.processed_data.index:
                    future_returns.append(
                        self.system.processed_data.loc[future_date, :].values
                    )
            
            if len(future_returns) > 0:
                avg_vol = np.nanmean([np.std(r) for r in future_returns])
                future_vols.append(avg_vol)
            else:
                future_vols.append(np.nan)
        
        densities = np.array(densities)
        future_vols = np.array(future_vols)
        
        # NaN ì œê±°
        valid_mask = ~np.isnan(future_vols)
        densities = densities[valid_mask]
        future_vols = future_vols[valid_mask]
        
        # ìƒìœ„/í•˜ìœ„ 30% ë¹„êµ
        high_threshold = np.percentile(densities, 70)
        low_threshold = np.percentile(densities, 30)
        
        high_density_mask = densities >= high_threshold
        low_density_mask = densities <= low_threshold
        
        high_vol = np.mean(future_vols[high_density_mask])
        low_vol = np.mean(future_vols[low_density_mask])
        
        print(f"\nğŸ“Š ë„¤íŠ¸ì›Œí¬ ë°€ë„ì™€ ì´í›„ ë³€ë™ì„± ê´€ê³„:\n")
        print(f"High connectivity days (ìƒìœ„ 30%):")
        print(f"  í‰ê·  ë„¤íŠ¸ì›Œí¬ ë°€ë„: {np.mean(densities[high_density_mask]):.4f}")
        print(f"  ì´í›„ {forward_days}ì¼ í‰ê·  ë³€ë™ì„±: {high_vol:.6f}\n")
        
        print(f"Low connectivity days (í•˜ìœ„ 30%):")
        print(f"  í‰ê·  ë„¤íŠ¸ì›Œí¬ ë°€ë„: {np.mean(densities[low_density_mask]):.4f}")
        print(f"  ì´í›„ {forward_days}ì¼ í‰ê·  ë³€ë™ì„±: {low_vol:.6f}\n")
        
        vol_ratio = high_vol / low_vol
        print(f"ë³€ë™ì„± ë¹„ìœ¨ (High/Low): {vol_ratio:.2f}x\n")
        
        print("âœ… ê¸°ì¤€: High > Low ì´ê³  ë¹„ìœ¨ > 1.2 ì´ë©´ ì˜ˆì¸¡ë ¥ í™•ì¸\n")
        
        return {
            'high_vol': high_vol,
            'low_vol': low_vol,
            'vol_ratio': vol_ratio
        }
    
    # =========================================================================
    # ì „ì²´ ê²€ì¦ ì‹¤í–‰
    # =========================================================================
    
    def run_all_tests(self):
        """ëª¨ë“  ê²€ì¦ ì‹¤í–‰"""
        print("\n" + "ğŸ”¬" * 40)
        print(" " * 15 + "ê²€ì¦ í”„ë ˆì„ì›Œí¬ ì‹œì‘")
        print("ğŸ”¬" * 40 + "\n")
        
        results = {}
        
        # 1. ì‹œê°„ì  ì„ í–‰ì„±
        results['temporal_validity'] = self.test_temporal_validity(forward_days=5, top_k=5)
        
        # 2. êµ¬ì¡° ì•ˆì •ì„±
        results['structural_stability'] = self.test_structural_stability(n_clusters=5)
        
        # 3. ê²½ì œì  ë…¼ë¦¬
        results['economic_plausibility'] = self.test_economic_plausibility()
        
        # 4. ë°˜ì‚¬ì‹¤ í…ŒìŠ¤íŠ¸
        results['perturbation'] = self.test_perturbation(target_asset='ì½”ìŠ¤í”¼200 ì—°ê²°')
        
        # 5. ê°„ì´ ì˜ˆì¸¡
        results['auxiliary_forecast'] = self.test_auxiliary_forecast(forward_days=5)
        
        print("\n" + "="*80)
        print("ğŸ¯ ì „ì²´ ê²€ì¦ ì™„ë£Œ")
        print("="*80)
        
        return results


if __name__ == "__main__":
    print("ì‹œìŠ¤í…œ ë¡œë”© ì¤‘...")
    
    # ì‹œìŠ¤í…œ ë¡œë“œ ë˜ëŠ” ìƒì„±
    system = MultiAssetCausalSystem(
        csv_path="ê°€ê²© ë°ì´í„°.csv",
        vol_window=20,
        causality_window=120,
        max_lag=3
    )
    
    # ìºì‹œì—ì„œ ë¡œë“œ ì‹œë„
    cached = MultiAssetCausalSystem.load_system("./results/system_cache.pkl")
    if cached is not None:
        system, _ = cached
        print("âœ“ ìºì‹œì—ì„œ ë¡œë“œ ì™„ë£Œ\n")
    else:
        print("ìºì‹œ ì—†ìŒ. ìƒˆë¡œ í•™ìŠµ ì¤‘...")
        system.load_and_preprocess()
        system.build_causal_networks(method='granger', sample_size=1000)
        system.analyze_market_structures()
        system.save_system("./results/system_cache.pkl")
        print("âœ“ í•™ìŠµ ì™„ë£Œ\n")
    
    # ê²€ì¦ ì‹¤í–‰
    validator = ValidationFramework(system)
    results = validator.run_all_tests()
